{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T21:22:52.481805Z",
     "start_time": "2025-02-24T21:22:52.479958Z"
    }
   },
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Byte Pair Encoding (BPE)",
   "id": "a6a24d1a75f6fe1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T21:22:52.504512Z",
     "start_time": "2025-02-24T21:22:52.499922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges):\n",
    "    # Initialize vocabulary with individual characters\n",
    "    vocab = {' '.join(word) + ' </w>': count for word, count in Counter(corpus.split()).items()}\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(f\"Iteration {i+1}: Merged {best}\")\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Example usage\n",
    "corpus = \"low lower lowest\"\n",
    "num_merges = 10\n",
    "vocab = byte_pair_encoding(corpus, num_merges)\n",
    "print(\"Final vocabulary:\", vocab)"
   ],
   "id": "91151a8ef33f2261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merged ('l', 'o')\n",
      "Iteration 2: Merged ('lo', 'w')\n",
      "Iteration 3: Merged ('low', 'e')\n",
      "Iteration 4: Merged ('low', '</w>')\n",
      "Iteration 5: Merged ('lowe', 'r')\n",
      "Iteration 6: Merged ('lower', '</w>')\n",
      "Iteration 7: Merged ('lowe', 's')\n",
      "Iteration 8: Merged ('lowes', 't')\n",
      "Iteration 9: Merged ('lowest', '</w>')\n",
      "Final vocabulary: {'low</w>': 1, 'lower</w>': 1, 'lowest</w>': 1}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T21:22:52.526175Z",
     "start_time": "2025-02-24T21:22:52.524491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the example corpus\n",
    "corpus = \"low lower lowest\""
   ],
   "id": "754d3caeab403937",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wordpiece Tokenization",
   "id": "7595a11b587bb803"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T21:22:52.540195Z",
     "start_time": "2025-02-24T21:22:52.535653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "def get_pairs(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def wordpiece_tokenization(corpus, vocab_size):\n",
    "    # Initialize vocabulary with individual characters\n",
    "    vocab = {' '.join(word) + ' </w>': count for word, count in Counter(corpus.split()).items()}\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        pairs = get_pairs(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print(f\"Merged {best_pair}\")\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 20\n",
    "vocab = wordpiece_tokenization(corpus, vocab_size)\n",
    "print(\"Final vocabulary:\", vocab)\n"
   ],
   "id": "d29a431af94deb58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged ('l', 'o')\n",
      "Merged ('lo', 'w')\n",
      "Merged ('low', 'e')\n",
      "Merged ('low', '</w>')\n",
      "Merged ('lowe', 'r')\n",
      "Merged ('lower', '</w>')\n",
      "Merged ('lowe', 's')\n",
      "Merged ('lowes', 't')\n",
      "Merged ('lowest', '</w>')\n",
      "Final vocabulary: {'low</w>': 1, 'lower</w>': 1, 'lowest</w>': 1}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T21:22:53.515967Z",
     "start_time": "2025-02-24T21:22:52.611304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "# Load pre-trained tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "bert_tokens = bert_tokenizer.tokenize(corpus)\n",
    "bert_ids = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "# Tokenize using RoBERTa tokenizer\n",
    "roberta_tokens = roberta_tokenizer.tokenize(corpus)\n",
    "roberta_ids = roberta_tokenizer.convert_tokens_to_ids(roberta_tokens)\n",
    "\n",
    "print(\"BERT Tokens:\", bert_tokens)\n",
    "print(\"BERT Token IDs:\", bert_ids)\n",
    "print(\"RoBERTa Tokens:\", roberta_tokens)\n",
    "print(\"RoBERTa Token IDs:\", roberta_ids)"
   ],
   "id": "b6e1ab3fd1e62d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokens: ['low', 'lower', 'lowest']\n",
      "BERT Token IDs: [2659, 2896, 7290]\n",
      "RoBERTa Tokens: ['low', 'Ġlower', 'Ġlowest']\n",
      "RoBERTa Token IDs: [5481, 795, 3912]\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
