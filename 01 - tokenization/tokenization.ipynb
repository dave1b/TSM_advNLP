{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.287853Z",
     "start_time": "2025-02-27T15:32:26.283597Z"
    }
   },
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Byte Pair Encoding (BPE)",
   "id": "a6a24d1a75f6fe1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.299730Z",
     "start_time": "2025-02-27T15:32:26.296452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the example corpus\n",
    "corpus = \"low low low low low lower lower newest newest newest newest newest newest widest widest widest\"\n",
    "example_sentence = \"lowering the newest wide\""
   ],
   "id": "754d3caeab403937",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.321915Z",
     "start_time": "2025-02-27T15:32:26.312293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_subword_pairs(vocab) -> dict:\n",
    "    \"\"\"Get frequency of adjacent subword pairs in the vocabulary.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in) -> dict:\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges) -> tuple[dict, list]:\n",
    "    # Initialize vocabulary with individual characters\n",
    "    vocab = {' '.join(word) + ' </w>': count for word, count in Counter(corpus.split()).items()}\n",
    "    final_vocab = []\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_subword_pairs(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        final_vocab.append(best[0] + best[1])\n",
    "        print(f\"Iteration {i + 1}: Merged {best}\")\n",
    "\n",
    "    return vocab, final_vocab\n",
    "\n",
    "num_merges = 5\n",
    "merged_vocab, final_bpe_vocab = byte_pair_encoding(corpus, num_merges)\n",
    "print(\"\\nFinal vocabulary:\", final_bpe_vocab)\n",
    "print(\"Merged vocabulary:\", merged_vocab)"
   ],
   "id": "91151a8ef33f2261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merged ('e', 's')\n",
      "Iteration 2: Merged ('es', 't')\n",
      "Iteration 3: Merged ('est', '</w>')\n",
      "Iteration 4: Merged ('l', 'o')\n",
      "Iteration 5: Merged ('lo', 'w')\n",
      "\n",
      "Final vocabulary: ['es', 'est', 'est</w>', 'lo', 'low']\n",
      "Merged vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wordpiece Tokenization",
   "id": "7595a11b587bb803"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.344360Z",
     "start_time": "2025-02-27T15:32:26.331823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_subword_pairs(vocab) -> dict:\n",
    "    \"\"\"Get frequency of adjacent subword pairs in the vocabulary.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in) -> dict:\n",
    "    \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")  # Match full token pairs\n",
    "\n",
    "    for word in v_in:\n",
    "        # Merge the pair into a single token\n",
    "        w_out = p.sub(\"\".join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out\n",
    "\n",
    "\n",
    "def compute_score(pair, pairs, vocab) -> float:\n",
    "    \"\"\"Compute the WordPiece score: P(xy) / (P(x) * P(y))\"\"\"\n",
    "    xy = pairs[pair]  # Frequency of merged token\n",
    "    x = sum(vocab[word] for word in vocab if pair[0] in word.split())\n",
    "    y = sum(vocab[word] for word in vocab if pair[1] in word.split())\n",
    "    return xy / (x * y) if x * y > 0 else 0  # Avoid division by zero\n",
    "\n",
    "\n",
    "def wordpiece_tokenization(corpus, vocab_size) -> tuple[dict, set]:\n",
    "    \"\"\"Train WordPiece tokenization.\"\"\"\n",
    "    # Step 1: Initialize vocabulary with characters + [UNK]\n",
    "    word_freqs = Counter(corpus.split())\n",
    "    vocab = {\" \".join(word) + \" </w>\": freq for word, freq in word_freqs.items()}\n",
    "    final_vocab = set(char for word in word_freqs for char in word) | {\"[UNK]\"}\n",
    "\n",
    "    while len(final_vocab) < vocab_size:\n",
    "        pairs = get_subword_pairs(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        # Step 2: Select merge based on WordPiece scoring function\n",
    "        best_pair = max(pairs, key=lambda p: compute_score(p, pairs, vocab))\n",
    "        merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "        # Add to vocabulary (use ## if not at word start)\n",
    "        if best_pair[0] in final_vocab:\n",
    "            merged_token = \"##\" + merged_token if not best_pair[0].endswith(\"</w>\") else merged_token\n",
    "        final_vocab.add(merged_token)\n",
    "\n",
    "        # Step 3: Merge the vocabulary\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print(f\"Merged: {best_pair} -> {merged_token}\")\n",
    "\n",
    "    return vocab, final_vocab\n",
    "\n",
    "vocab_size = 20\n",
    "merged_vocab, final_wp_vocab = wordpiece_tokenization(corpus, vocab_size)\n",
    "\n",
    "print(\"\\nFinal vocabulary:\", final_wp_vocab)\n",
    "print(\"Merged vocabulary:\", merged_vocab)"
   ],
   "id": "26536c8364137827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: ('i', 'd') -> ##id\n",
      "Merged: ('l', 'o') -> ##lo\n",
      "Merged: ('s', 't') -> ##st\n",
      "Merged: ('e', 'r') -> ##er\n",
      "Merged: ('n', 'e') -> ##ne\n",
      "Merged: ('e', 'st') -> ##est\n",
      "Merged: ('id', 'est') -> idest\n",
      "Merged: ('lo', 'w') -> low\n",
      "Merged: ('low', 'er') -> ##lower\n",
      "\n",
      "Final vocabulary: {'##est', 'low', 's', 'n', 't', 'o', '##lo', 'idest', '##er', 'r', 'i', '##ne', 'e', '[UNK]', '##lower', '##st', '##id', 'd', 'w', 'l'}\n",
      "Merged vocabulary: {'low </w>': 5, 'lower </w>': 2, 'ne w est </w>': 6, 'w idest </w>': 3}\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenize sequence with vocabulary",
   "id": "a7b12996d3b2de4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.358213Z",
     "start_time": "2025-02-27T15:32:26.353136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_word_for_bpe(word, start_char_index, vocab) -> tuple[str, int]:\n",
    "    start_char = word[start_char_index]\n",
    "    end_char_index = None\n",
    "    chars = \"\"\n",
    "    token = None\n",
    "    index = start_char_index\n",
    "    for char in word[start_char_index:]:\n",
    "        index += 1\n",
    "        chars += char\n",
    "        if chars in vocab or chars == \"</w>\":\n",
    "            token = chars\n",
    "            end_char_index = index\n",
    "    if token is None:\n",
    "        return start_char, start_char_index + 1\n",
    "    else:\n",
    "        return token, end_char_index\n",
    "\n",
    "\n",
    "def apply_bpe_tokenizer(sequence, vocab) -> list:\n",
    "    encoded = []\n",
    "    words = [word + \"</w>\" for word in sequence.split()]\n",
    "\n",
    "    for word in words:\n",
    "        word_char_index = 0\n",
    "        while word_char_index is not len(word):\n",
    "            token, word_char_index = tokenize_word_for_bpe(word, word_char_index, vocab)\n",
    "            encoded.append(token)\n",
    "\n",
    "    return encoded"
   ],
   "id": "24c177fb49a8ac33",
   "outputs": [],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.371754Z",
     "start_time": "2025-02-27T15:32:26.366772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_word_for_wp(word, start_char_index, vocab) -> tuple[str, int]:\n",
    "    start_char = word[start_char_index]\n",
    "    end_char_index = None\n",
    "    chars = \"##\"\n",
    "    token = None\n",
    "    index = start_char_index\n",
    "    for char in word[start_char_index:]:\n",
    "        index += 1\n",
    "        chars += char\n",
    "        if chars in vocab or chars == \"</w>\":\n",
    "            token = chars\n",
    "            end_char_index = index\n",
    "    if token is None:\n",
    "        return start_char, start_char_index + 1\n",
    "    else:\n",
    "        return token, end_char_index\n",
    "\n",
    "def apply_wp_tokenizer(sequence, vocab) -> list:\n",
    "    encoded = []\n",
    "    words = [word for word in sequence.split()]\n",
    "    for word in words:\n",
    "            word_char_index = 0\n",
    "            while word_char_index is not len(word):\n",
    "                token, word_char_index = tokenize_word_for_wp(word, word_char_index, vocab)\n",
    "                encoded.append(token)\n",
    "    return encoded"
   ],
   "id": "127ba0387fab3add",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Other implemplementations\n",
    "\n",
    "### BPE"
   ],
   "id": "2c951707e09d40a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.385832Z",
     "start_time": "2025-02-27T15:32:26.380621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "bpw_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Use whitespace to split words\n",
    "bpw_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train on a corpus\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "bpw_tokenizer.train_from_iterator([corpus], trainer=trainer)\n",
    "\n",
    "# Show the final vocab\n",
    "vocab = bpw_tokenizer.get_vocab()\n",
    "print(\"Final vocab:\", vocab)\n",
    "\n",
    "# Tokenize a sentence\n",
    "tokenized = bpw_tokenizer.encode(example_sentence)\n",
    "print(f\"\\nTokenization of '{example_sentence}': {tokenized.tokens}\")"
   ],
   "id": "d258e0ee1f33febd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Final vocab: {'[SEP]': 2, 'd': 5, 'n': 9, 'new': 20, 'est': 16, '[CLS]': 1, 'r': 11, 'e': 6, 'newest': 21, 'l': 8, 'es': 15, 'idest': 23, 't': 13, 'widest': 24, '[PAD]': 3, 'ew': 19, 'lo': 17, 'lower': 26, 'dest': 22, 'low': 18, 'w': 14, '[MASK]': 4, '[UNK]': 0, 's': 12, 'o': 10, 'er': 25, 'i': 7}\n",
      "\n",
      "Tokenization of 'lowering the newest wide': ['lower', 'i', 'n', 't', 'e', 'newest', 'w', 'i', 'd', 'e']\n"
     ]
    }
   ],
   "execution_count": 206
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### WordPiece",
   "id": "35f8a4b66da1d3e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:26.406869Z",
     "start_time": "2025-02-27T15:32:26.401816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# Initialize WordPiece tokenizer\n",
    "wp_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "wp_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train on a corpus\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "wp_tokenizer.train_from_iterator([corpus], trainer=trainer)\n",
    "\n",
    "# Show the final vocab\n",
    "vocab = wp_tokenizer.get_vocab()\n",
    "print(\"Final vocab:\", vocab)\n",
    "\n",
    "# Tokenize a sentence\n",
    "tokenized = wp_tokenizer.encode(example_sentence)\n",
    "print(f\"\\nTokenization of '{example_sentence}': {tokenized.tokens}\")"
   ],
   "id": "7356010e1de416c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Final vocab: {'[CLS]': 1, '##d': 22, 'i': 7, '##e': 15, '[UNK]': 0, '##west': 28, 'e': 6, '##dest': 31, 'n': 9, '##s': 17, '##i': 21, 'newest': 29, 't': 13, '[MASK]': 4, 'lower': 34, '##est': 24, '[SEP]': 2, 'wi': 30, 'o': 10, 'w': 14, 'r': 11, '##o': 19, 'l': 8, '##r': 20, 'low': 26, 'widest': 32, '##t': 18, '##w': 16, '##es': 23, 'ne': 27, 'd': 5, 'lo': 25, '##er': 33, '[PAD]': 3, 's': 12}\n",
      "\n",
      "Tokenization of 'lowering the newest wide': ['[UNK]', '[UNK]', 'newest', 'wi', '##d', '##e']\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hugging Face Implementations\n",
    "BertTokenizer, RobertaTokenizer"
   ],
   "id": "9a0298ff49390030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:27.107369Z",
     "start_time": "2025-02-27T15:32:26.411165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "# Load pre-trained tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "bert_tokens = bert_tokenizer.tokenize(example_sentence)\n",
    "\n",
    "# Tokenize using RoBERTa tokenizer\n",
    "roberta_tokens = roberta_tokenizer.tokenize(example_sentence)\n",
    "\n",
    "print(\"BERT Tokens:\", bert_tokens)\n",
    "print(\"RoBERTa Tokens:\", roberta_tokens)"
   ],
   "id": "b6e1ab3fd1e62d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokens: ['lowering', 'the', 'newest', 'wide']\n",
      "RoBERTa Tokens: ['lower', 'ing', 'Ġthe', 'Ġnewest', 'Ġwide']\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare the tokenization using different tokenizers",
   "id": "d5d72dbcb288195b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:32:27.122315Z",
     "start_time": "2025-02-27T15:32:27.117227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Original Sentence: {example_sentence}\")\n",
    "\n",
    "custom_bpe_tokens = apply_bpe_tokenizer(example_sentence, final_bpe_vocab)\n",
    "custom_wordpiece_tokens = apply_wp_tokenizer(example_sentence, final_wp_vocab)\n",
    "\n",
    "lib_bpe_tokenized = bpw_tokenizer.encode(example_sentence)\n",
    "lib_wp_tokenized = wp_tokenizer.encode(example_sentence)\n",
    "\n",
    "bert_tokens = bert_tokenizer.tokenize(example_sentence)\n",
    "roberta_tokens = roberta_tokenizer.tokenize(example_sentence)\n",
    "\n",
    "print(\"\\nCustom BPE Tokens:\", custom_bpe_tokens)\n",
    "print(\"Library BPE Tokens:\", lib_bpe_tokenized.tokens)\n",
    "\n",
    "print(\"\\nCustom WordPiece Tokens:\", custom_wordpiece_tokens)\n",
    "print(\"Library WordPiece Tokens:\", lib_wp_tokenized.tokens)\n",
    "print(\"\\nBERT Tokens:\", bert_tokens)\n",
    "print(\"RoBERTa Tokens:\", roberta_tokens)"
   ],
   "id": "469636d8c01a86e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: lowering the newest wide\n",
      "\n",
      "Custom BPE Tokens: ['low', 'e', 'r', 'i', 'n', 'g', '</w>', 't', 'h', 'e', '</w>', 'n', 'e', 'w', 'est</w>', 'w', 'i', 'd', 'e', '</w>']\n",
      "Library BPE Tokens: ['lower', 'i', 'n', 't', 'e', 'newest', 'w', 'i', 'd', 'e']\n",
      "\n",
      "Custom WordPiece Tokens: ['##lower', 'i', 'n', 'g', 't', 'h', 'e', '##ne', 'w', '##est', 'w', '##id', 'e']\n",
      "Library WordPiece Tokens: ['[UNK]', '[UNK]', 'newest', 'wi', '##d', '##e']\n",
      "\n",
      "BERT Tokens: ['lowering', 'the', 'newest', 'wide']\n",
      "RoBERTa Tokens: ['lower', 'ing', 'Ġthe', 'Ġnewest', 'Ġwide']\n"
     ]
    }
   ],
   "execution_count": 209
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
